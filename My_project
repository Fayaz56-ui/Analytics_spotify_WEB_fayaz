I am creating simple Pipe line SQL,PYTHON,HADOOP,HIVE,SQOOP,AIRFLOW
-------------------------------------------------------------------------------------------------------------------------------
Extract (from a database to Hadoop)
Transform (using Python and Hive)
Load (into a final destination)
This pipeline will have:
---------------------------------------------------------------------------------------------------------------------------------
SQL for querying the source database
Python for data transformation
Hadoop as a data processing framework
Hive for querying the processed data
Sqoop for transferring data from SQL to Hadoop
Airflow to orchestrate the entire ETL workflow.
-------------------------------------------------------------------------------------------------------------------------------------
Prerequisites
Ensure the following tools and environments are set up before implementing the pipeline:

Hadoop Cluster (with HDFS running)
Hive installed and configured
Apache Airflow installed and configured
Sqoop installed to connect Hadoop with SQL-based systems (like MySQL, PostgreSQL, etc.)
Python installed along with necessary libraries (pandas, pyhive, etc.)
------------------------------------------------------------------------------------------------------------------------------------------
Step-by-Step ETL Pipeline
1. Extract (SQL to Hadoop using Sqoop)
The first step is to extract data from a relational database (e.g., MySQL) into Hadoop Distributed File System (HDFS).

SQL Example (MySQL Query):

(CREATE TABLE AND INSERT THIS BELOW DATA INTO THE MYSQL)

CREATE TABLE customers (
    id INT AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(100),
    join_date DATE
);


INSERT INTO customers (name, email, join_date)
VALUES
    ('John Doe', 'john.doe@example.com', '2021-05-12'),
    ('Jane Smith', 'jane.smith@example.com', '2022-08-21'),
    ('Bob Johnson', 'bob.johnson@example.com', '2020-11-15'),
    ('Alice Williams', 'alice.williams@example.com', '2019-01-10'),
    ('Charlie Brown', 'charlie.brown@example.com', '2023-03-30'),
    ('David Wilson', 'david.wilson@example.com', '2021-07-25'),
    ('Eve Davis', 'eve.davis@example.com', '2022-02-17'),
    ('Frank Miller', 'frank.miller@example.com', '2020-09-30'),
    ('Grace Lee', 'grace.lee@example.com', '2023-06-11'),
    ('Hannah Taylor', 'hannah.taylor@example.com', '2022-12-05');

------------------------------------------------
sql
Copy code
SELECT * FROM customers;
------------------------------------------------
Use Sqoop to import this data from MySQL into HDFS.

Sqoop Command:

bash
Copy code
sqoop import \
  --connect jdbc:mysql://<mysql_host>:<port>/<database_name> \
  --username <username> \
  --password <password> \
  --table customers \
  --target-dir /user/hadoop/customers_data
This command extracts the data from the MySQL customers table and loads it into a directory in HDFS.
-----------------------------------------------------------------------------

2. Transform (using Python and Hive)
After extracting the data, we need to process and transform it. You can use Hive to query the data and Python to perform additional transformations.

First, create a Hive table to access the data stored in HDFS:

sql
Copy code
CREATE EXTERNAL TABLE customers_raw (
  id INT,
  name STRING,
  email STRING,
  join_date STRING
)
STORED AS TEXTFILE
LOCATION '/user/hadoop/customers_data';
Now, use Hive to filter or clean the data:

sql
Copy code
CREATE TABLE customers_transformed AS
SELECT id, name, email, TO_DATE(join_date) AS join_date
FROM customers_raw
WHERE join_date >= '2020-01-01';
Use Python for additional transformations if necessary:

python
Copy code
import pandas as pd
from pyhive import hive

# Connect to Hive
conn = hive.connect(host='hive_host', port=10000, username='hadoop')
cursor = conn.cursor()

# Load the data into a Pandas DataFrame
cursor.execute('SELECT * FROM customers_transformed')
data = cursor.fetchall()

df = pd.DataFrame(data, columns=['id', 'name', 'email', 'join_date'])

# Perform some Python transformations
df['email_domain'] = df['email'].apply(lambda x: x.split('@')[1])

# Save the transformed data back to HDFS or a file
df.to_csv('/user/hadoop/customers_transformed.csv', index=False)

---------------------------------------------------------------------------------------------------------------------------------
3. Load (into final destination)
Once the transformation is complete, you can load the data into the final destination. For example, you can load the transformed data into a final Hive table or an external database.

Load into Hive:

sql
Copy code
LOAD DATA INPATH '/user/hadoop/customers_transformed.csv' INTO TABLE customers_final;
-----------------------------------------------------------------------------------------------------------------------------
4. Orchestrating with Apache Airflow
Airflow can be used to orchestrate the ETL workflow. First, define tasks and their dependencies in an Airflow DAG (Directed Acyclic Graph).

Airflow DAG Example (Python Script):

python
Copy code
from airflow import DAG
from airflow.providers.sqoop.operators.sqoop import SqoopOperator
from airflow.providers.hive.operators.hive import HiveOperator
from airflow.providers.python.operators.python import PythonOperator
from datetime import datetime

def transform_data():
    # Python transformation code here (as shown above)
    pass

default_args = {
    'owner': 'airflow',
    'retries': 3,
    'start_date': datetime(2025, 1, 1),
}

dag = DAG('etl_pipeline', default_args=default_args, schedule_interval='@daily')
--------------------------------------------------------------------------------------------------------------------

# Task 1: Extract data from MySQL to HDFS using Sqoop
extract_task = SqoopOperator(
    task_id='extract_data',
    conn_id='mysql_conn',  # Connection ID for MySQL in Airflow
    command="sqoop import --connect jdbc:mysql://<mysql_host>/<db_name> --table customers --target-dir /user/hadoop/customers_data",
    dag=dag
)

# Task 2: Transform data in Hive
transform_task = HiveOperator(
    task_id='transform_data',
    hql="""
    CREATE TABLE customers_transformed AS
    SELECT id, name, email, TO_DATE(join_date) AS join_date
    FROM customers_raw
    WHERE join_date >= '2020-01-01';
    """,
    hive_cli_conn_id='hive_conn',  # Hive connection ID in Airflow
    dag=dag
)

# Task 3: Run Python transformation
python_transform_task = PythonOperator(
    task_id='python_transform',
    python_callable=transform_data,
    dag=dag
)

# Task 4: Load data into final Hive table
load_task = HiveOperator(
    task_id='load_data',
    hql="""
    LOAD DATA INPATH '/user/hadoop/customers_transformed.csv' INTO TABLE customers_final;
    """,
    hive_cli_conn_id='hive_conn',
    dag=dag
)

-----------------------------------------------------------------------------------------------

# Set task dependencies
extract_task >> transform_task >> python_transform_task >> load_task
------------------------------------------------------------------------------------------------
Explanation:
Extract: The SqoopOperator extracts data from MySQL to HDFS.
Transform: The HiveOperator transforms the data by applying SQL operations in Hive, and PythonOperator applies any further transformations.
Load: The transformed data is loaded back into a Hive table or any other final destination.
Airflow: Orchestrates the entire ETL process by defining the sequence of tasks.
Final Notes:
Customize the transformations based on your data needs.
You can set up different schedulers and triggers in Airflow based on your requirements.
Ensure to handle error logging, retries, and task dependencies effectively for a robust pipeline.
This is a simplified ETL pipeline using SQL, Python, Hadoop, Hive, Sqoop, and Apache Airflow.

------------------------------------------------------------------------------------------------------------------------------------------------------

                                                                           end.
